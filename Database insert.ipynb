{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load all seasons and insert to postgresql database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It also handles stage unique end dates, which is critical to derive league table from fixtures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import logging\n",
    "from os import listdir\n",
    "import os.path\n",
    "from os.path import isfile, join\n",
    "import psycopg2\n",
    "from sqlalchemy import create_engine\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user = 'postgres'\n",
    "password = 'postgres'\n",
    "databasename = 'soccer-db'\n",
    "host = 'localhost'\n",
    "port = '5432'\n",
    "\n",
    "root_path = 'L:/Dev/Sandbox/Apps Development/SoccerStats/historical result files/France'\n",
    "\n",
    "# Only ligue1 has been treated so far\n",
    "csv_root_path_ligue1 = os.path.join(root_path,'ligue1')\n",
    "\n",
    "# Here we could add other championship links...\n",
    "# To do"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logger = logging.getLogger('myAppDbInsert')\n",
    "hdlr = logging.FileHandler(os.path.join(root_path,'ligue1comDbInsertion.log'))\n",
    "formatter = logging.Formatter('%(asctime)s %(levelname)s %(message)s')\n",
    "hdlr.setFormatter(formatter)\n",
    "logger.addHandler(hdlr) \n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "# close log file objects\n",
    "#logger.removeHandler(hdlr)\n",
    "#del logger,hdlr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [db_connect] connect to postgres db and give back conn object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def db_connect():\n",
    "    try:\n",
    "        conn = psycopg2.connect(\"dbname='{0}' user='{1}' host='{2}' password='{3}'\".format(databasename, user, host, password))\n",
    "    except:\n",
    "        print(\"I am unable to connect to the database\")\n",
    "\n",
    "    # Open a cursor to perform database operations\n",
    "    return conn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [db_seasons_table_to_dic] convert seasons table to dictionary {name:id}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def db_seasons_table_to_dic(cur):\n",
    "    cur.execute(\"select * from seasons\")\n",
    "    rows = cur.fetchall()\n",
    "    dic = {}\n",
    "    for row in rows:\n",
    "        dic[row[1]] = row[0]\n",
    "    return dic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [db_allcompetitions_table_to_dic] convert allcompetitions table to dictionary {name:id}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def db_allcompetitions_table_to_dic(cur):\n",
    "    cur.execute(\"select * from allcompetitions\")\n",
    "    rows = cur.fetchall()\n",
    "    dic = {}\n",
    "    for row in rows:\n",
    "        dic[row[1]] = row[0]\n",
    "    return dic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [db_competitions_table_to_dic] convert competitions table to dictionary {name:id}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def db_competitions_table_to_dic(cur, competitionname):\n",
    "    cur.execute(\"select * from competitionid('{0}')\".format(competitionname))\n",
    "    rows = cur.fetchall()\n",
    "    dic = {}\n",
    "    for row in rows:\n",
    "        dic[row[0]] = row[1]\n",
    "    return dic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [db_stages_table_to_dic] convert stages table to dictionary {name:id}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def db_stages_table_to_dic(cur):\n",
    "    cur.execute(\"select * from stages\")\n",
    "    rows = cur.fetchall()\n",
    "    dic = {}\n",
    "    for row in rows:\n",
    "        dic[row[1]] = row[0]\n",
    "    return dic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [db_teams_table_to_dic] convert teams table to dictionary {name:id}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def db_teams_table_to_dic(cur):\n",
    "    cur.execute(\"select * from teams\")\n",
    "    rows = cur.fetchall()\n",
    "    dic = {}\n",
    "    for row in rows:\n",
    "        dic[row[1]] = row[0]\n",
    "    return dic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [db_fixturestatus_table_to_dic] convert fixturestatus table to dictionary {name:id}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def db_fixturestatus_table_to_dic(cur):\n",
    "    cur.execute(\"select * from fixturestatus\")\n",
    "    rows = cur.fetchall()\n",
    "    dic = {}\n",
    "    for row in rows:\n",
    "        dic[row[1]] = row[0]\n",
    "    return dic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [db_get_seasons_name] get list of seasons name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def db_get_seasons(cur):\n",
    "    cur.execute(\"select name from seasons\")\n",
    "    rows = cur.fetchall()\n",
    "    return [x[0] for x in rows]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [db_get_stages_name] get list of stages name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def db_get_stages(cur):\n",
    "    cur.execute(\"select name from stages\")\n",
    "    rows = cur.fetchall()\n",
    "    return [x[0] for x in rows]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [db_get_fixtures_max_index] return max index from fixtures table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def db_get_fixtures_max_index(cur):\n",
    "    cur.execute(\"select max(id) from fixtures\")\n",
    "    rows = cur.fetchall()\n",
    "    if len(rows) != 1:\n",
    "        # there should be exactly one row of result, a tuple (x,y) actually\n",
    "        return -1\n",
    "    # this is a tuple (x,y)\n",
    "    res = rows[0]\n",
    "    \n",
    "    if res[0] == None:\n",
    "        # tuple is (None,) -> table must be empty\n",
    "        return 1\n",
    "    \n",
    "    # table not empty, return the actual max index\n",
    "    return res[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [db_week_dates_to_df] call to week_dates database function, which return dates associated to each stage of type \"week\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def db_week_dates_to_df(cur, competition, season):\n",
    "    cur.execute(\"select * from week_dates('{0}', '{1}')\".format(competition, season))\n",
    "    colnames = [desc[0] for desc in cur.description]\n",
    "    \n",
    "    df = pd.DataFrame(data=cur.fetchall(), columns=colnames)\n",
    "    \n",
    "    # is there an override define for this season?\n",
    "    key = competition.replace(' ','').lower() + '_' + season.replace('/','_')\n",
    "    df = handle_week_override(df, key)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [get_delayed_fixtures_to_df] get delayed fixtures for given competition/season, returns dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# override is a dictionary defining a list of week stage to substitute for each competition/season\n",
    "# for instance: {'Ligue1_2016/2017': [('Week 11', 'Week 14')]}\n",
    "# means 'Week 11' is currently wrongly place after 'Week 14'\n",
    "# we need to decrement all weekid from Week 12 to week 14, and increment 'Week 11' by 3 in that case (14-11)\n",
    "# that's useful for messed up dates\n",
    "ovrd = {\n",
    "    'ligue1_1970_1971': [('Week 11', 'Week 14')],\n",
    "    'ligue1_1972_1973': [('Week 17', 'Week 29')],\n",
    "    'ligue1_1977_1978': [('Week 15', 'Week 17')],\n",
    "    'ligue1_1984_1985': [('Week 13', 'Week 14'), ('Week 13', 'Week 14'), ('Week 22', 'Week 27')],\n",
    "    'ligue1_2009_2010': [('Week 11', 'Week 17')]\n",
    "}\n",
    "\n",
    "# substitute week1 and week2 in dataframe, supposing week 2 is currently wrongly after week 3\n",
    "def substitute_weeks_in_df(df, week1, week2):\n",
    "    \n",
    "    # get week ids\n",
    "    i1 = int(week1[-2:])\n",
    "    i2 = int(week2[-2:])\n",
    "    diff = i2 - i1\n",
    "    \n",
    "    # dummy values used for replacement\n",
    "    tmp = 'Week XX'\n",
    "    tmp_start = 'Week SS'\n",
    "    \n",
    "    # first, we replace the troublesome week id with a dummy value to keep track of it later\n",
    "    df['stage'].replace(['Week ' + \"%02d\" % (i1,)], [tmp_start], inplace=True)\n",
    "    \n",
    "    for i in range(i1 + 1, i2 + 1):\n",
    "        \n",
    "        curr_week = 'Week ' + \"%02d\" % (i,)\n",
    "        # we want to decrement week id here:\n",
    "        new_week = 'Week ' + \"%02d\" % (i - 1,)\n",
    "        \n",
    "        # actual replacement here:\n",
    "        df['stage'].replace([curr_week], [tmp], inplace=True)\n",
    "        df['stage'].replace([new_week], [curr_week], inplace=True)\n",
    "        df['stage'].replace([tmp], [new_week], inplace=True)\n",
    "    \n",
    "    \n",
    "    # finally, we increment week id by the diff\n",
    "    df['stage'].replace([tmp_start], ['Week ' + \"%02d\" % (i1 + diff,)], inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def handle_week_override(df, key):\n",
    "    if(key in ovrd):\n",
    "        for tuple in ovrd[key]:\n",
    "            df = substitute_weeks_in_df(df, tuple[0], tuple[1])\n",
    "    return df\n",
    "\n",
    "def get_delayed_fixtures_to_df(cur, competition, season):\n",
    "    df = db_week_dates_to_df(cur, competition, season)\n",
    "    df2 = pd.DataFrame(columns=df.columns)\n",
    "    current_stage_id = 1\n",
    "    next_stage_id = 2\n",
    "    i = 0\n",
    "    j = 1\n",
    "    for stage in df['stage'][1:]:\n",
    "        stage_id = int(stage[-2:])\n",
    "        if(stage_id != current_stage_id and stage_id != next_stage_id):\n",
    "            df2.loc[i] = df.loc[j]\n",
    "            i = i+1\n",
    "        elif(stage_id == next_stage_id):\n",
    "            current_stage_id += 1\n",
    "            next_stage_id += 1\n",
    "        j = j+1\n",
    "    return df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [get_week_unique_dates_to_df] get week unique start/end dates for given competition/season, returns dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_week_unique_dates_to_df(cur, competition, season):\n",
    "    df1 = get_delayed_fixtures_to_df(cur, competition, season)\n",
    "    df2 = db_week_dates_to_df(cur, competition, season)\n",
    "    df3 = df2.merge(df1, on=['season','competition','stage', 'date'], how='left')\n",
    "    df4 = df3[df3.nb_games_y.isnull()][['stage','date']]\n",
    "    df5 = df4.groupby(['stage'], sort=False)['date'].max()\n",
    "    df6 = df4.groupby(['stage'], sort=False)['date'].min()\n",
    "    return pd.DataFrame({'start_date': df6, 'end_date': df5}).reset_index()[['stage','start_date','end_date']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [process_seasons_week_unique_dates] process all seasons for week unique start/end dates insertion to database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def check_week_unique_dates_df(df):\n",
    "    \n",
    "    correct_len = [34, 37, 38, 40]\n",
    "    \n",
    "    # check df len\n",
    "    if(df.shape[0] not in correct_len):\n",
    "        logger.error(\"Dataframe length not expected: {0}\".format(df.shape[0]))\n",
    "        return False\n",
    "    \n",
    "    # verify weeks and dates are ordered ASC\n",
    "    prev_end_date = df.iloc[0]['end_date']\n",
    "    for i in range(1, df.shape[0]):\n",
    "        curr_end_date = df.iloc[i]['end_date']\n",
    "        \n",
    "        # messed up dates check\n",
    "        if(prev_end_date > df.iloc[i]['start_date'] or df.iloc[i]['start_date'] > curr_end_date):\n",
    "            logger.error(\"Dataframe dates are messed up in row {0}, for start_date={1}\".format(i, df.iloc[i]['start_date']))\n",
    "            return False\n",
    "        \n",
    "        # big gap dates check (between previous and current week)\n",
    "        day_diff = (prev_end_date - df.iloc[i]['start_date']).days\n",
    "        if(day_diff > 30):\n",
    "            logger.error(\"There is a big gap of {0} days between 2 weeks in the dataframe: start_date={1}, end_date={2}\".format(day_diff, df.iloc[i]['start_date'], df.iloc[i]['end_date']))\n",
    "            return False\n",
    "        \n",
    "        prev_end_date = curr_end_date\n",
    "    return True\n",
    "\n",
    "def process_seasons_week_unique_dates(competition):\n",
    "    \n",
    "    # connect to database\n",
    "    conn = db_connect()\n",
    "    cur = conn.cursor()\n",
    "    \n",
    "    # get seasons list\n",
    "    seasons = db_get_seasons(cur)\n",
    "    \n",
    "    # get stages and competitions dic from db\n",
    "    dic_stages = db_stages_table_to_dic(cur)\n",
    "    dic_competitions = db_competitions_table_to_dic(cur, competition)\n",
    "    \n",
    "    for season in seasons:\n",
    "        \n",
    "        # get week unique dates df\n",
    "        df = get_week_unique_dates_to_df(cur, competition, season)\n",
    "        \n",
    "        if(check_week_unique_dates_df(df)):\n",
    "            \n",
    "            # normalize\n",
    "            df['competitionid'] = dic_competitions[season]\n",
    "            df['stageid'] = [dic_stages[x] for x in df['stage']]\n",
    "            df = df[['competitionid','stageid', 'start_date', 'end_date']]\n",
    "            \n",
    "            # insert week dates to database\n",
    "            #db_insert_to_table(df, 'stage_dates')\n",
    "            print('Week dates inserted to db for {0}, season {1}..'.format(competition, season))\n",
    "            \n",
    "        else:\n",
    "            # log issue\n",
    "            logger.error(\"Dataframe didn't pass the check for {0}, season {1}\".format(competition, season))\n",
    "            \n",
    "    # close connection\n",
    "    conn.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [histo_csv_to_df] load all csv files and convert into pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def histo_csv_to_df(root_path):\n",
    "    \n",
    "    csvfiles = [f for f in listdir(root_path) if isfile(join(root_path, f)) and f.lower().endswith('.csv')]\n",
    "    \n",
    "    d = {}\n",
    "    data = []\n",
    "    firstfile = True\n",
    "    column_names = []\n",
    "    \n",
    "    for csvfile in csvfiles:\n",
    "\n",
    "        # open csv file\n",
    "        fname = join(root_path, csvfile)\n",
    "        with open(fname) as f:\n",
    "            content = f.readlines()\n",
    "        \n",
    "        # only for first file\n",
    "        if firstfile:\n",
    "            column_names = [x.strip() for x in content[0].split(',')]\n",
    "            firstfile = False\n",
    "        \n",
    "        # clean data\n",
    "        content = [x.strip() for x in content]\n",
    "        content = content[1:]\n",
    "\n",
    "        # explode rows into list\n",
    "        for item in content:\n",
    "            data += [item.split(',')]\n",
    "\n",
    "    return pd.DataFrame(columns=column_names, data=data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [get_competitions_to_df] construct competitions df out of fixtures df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_competitions_to_df(df_fixtures, dic_seasons, allcompetitionid):\n",
    "    df = pd.DataFrame(df_fixtures.season.unique(), columns=['season'])\n",
    "    df['seasonid'] = [dic_seasons[x] for x in df['season']]\n",
    "    df['allcompetitionid'] = allcompetitionid\n",
    "    return df[['seasonid', 'allcompetitionid']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [get_registered_teams] get registered teams for each competitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_registered_teams(df_fixtures_normalized, dic_stages):\n",
    "    \n",
    "    cols = ['competitionid', 'teamid']\n",
    "    df = pd.DataFrame({'competitionid':[],'teamid':[]},columns=cols, dtype=int)\n",
    "    for competitionid in df_fixtures_normalized.competitionid.unique():\n",
    "        \n",
    "        # get only rows for this competitionid\n",
    "        df_competitionid = df_fixtures_normalized.loc[df_fixtures_normalized['competitionid'] == competitionid]\n",
    "        \n",
    "        # remove playoffs if any\n",
    "        df_competitionid = df_competitionid.loc[df_competitionid['stageid'] != dic_stages['Relegation Playoff']]\n",
    "                \n",
    "        # get unique teamid among hometeamdid and awayteamid cols\n",
    "        unique_teamids = np.unique(df_competitionid[['hometeamid','awayteamid']])\n",
    "        \n",
    "        # create a new dataframe to wrap it up\n",
    "        new_df = pd.DataFrame(columns=cols)\n",
    "        new_df['teamid'] = [x for x in unique_teamids]\n",
    "        new_df['competitionid'] = competitionid\n",
    "        df = df.append(new_df)\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [analyse_histo] analyse dataframe resulting from preceding function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def analyse_histo(df):\n",
    "    \n",
    "    # get distinct seasons\n",
    "    seasons = list(df.season.unique())\n",
    "    \n",
    "    # remove playoff from the dataframe\n",
    "    df_no_playoff = df.loc[df['week'] != 'Play-off']\n",
    "    \n",
    "    # for each season\n",
    "    for season in seasons:\n",
    "        \n",
    "        # filter on season\n",
    "        df_tmp = df_no_playoff.loc[df['season'] == season]\n",
    "        \n",
    "        # get distinct # teams\n",
    "        nb_team = len(df_tmp.team_home.unique())\n",
    "        \n",
    "        # estimate # games\n",
    "        nb_game_estim = (nb_team -1) * 2 * nb_team / 2\n",
    "        \n",
    "        # get actual # games from dataframe and check it matches estimation\n",
    "        nb_game = len(df_tmp)\n",
    "        \n",
    "        # print results\n",
    "        diff = nb_game - nb_game_estim\n",
    "        status = diff == 0\n",
    "        print(\"Season {0}: nb_game_estim = {1}, nb_game = {2}, diff = {3}, status: {4}\".format(season,nb_game_estim, nb_game, diff,status))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [normalize_data] normalize dataframe to get a clean dataset before insertion to database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we need to find a much better way to handle all that (using unicode)\n",
    "def handle_accent(s):\n",
    "    s = s.replace(\"ã¨\", \"e\")\n",
    "    s = s.replace(\"ã©\", \"e\")\n",
    "    s = s.replace(\"ã¯\", \"i\")\n",
    "    s = s.replace(\"ã\", \"a\")\n",
    "    s = s.replace(\"a«\", \"e\")\n",
    "    s = s.replace(\"a¢\", \"a\")\n",
    "    s = s.replace(\"a§\", \"c\")\n",
    "    s = s.replace(\"ê\", \"e\")\n",
    "    s = s.replace(\"î\", \"i\")\n",
    "    s = s.replace(\"â\", \"a\")\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def format_team(team):\n",
    "    team = handle_accent(team)\n",
    "    if team == \"LOSC\":\n",
    "        return \"Lille OSC\"\n",
    "    elif team == \"GF38\":\n",
    "        return \"Grenoble Foot 38\"\n",
    "    else:\n",
    "        return team"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def format_stage(stage):\n",
    "    if stage[:4] == 'Week':\n",
    "        id = stage[4:]\n",
    "        if len(id) == 1:\n",
    "            id = '0' + id\n",
    "        return 'Week ' + id\n",
    "    elif stage == 'Play-off':\n",
    "        return 'Relegation Playoff'\n",
    "    return stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize_data(df_fixtures, dic_competitions, dic_stages, dic_fixturestatus, fixturestatus, dic_teams, fixtures_max_index):\n",
    "    \n",
    "    # create competitionid column from season name and dic competitions {seasonname:competitionid}\n",
    "    df_fixtures['competitionid'] = [dic_competitions[x] for x in df_fixtures['season']]\n",
    "    \n",
    "    # format stages to match database format\n",
    "    df_fixtures['stage'] = [format_stage(x) for x in df_fixtures['week']]\n",
    "    \n",
    "    # replace stages by their ids\n",
    "    df_fixtures['stageid'] = [dic_stages[x] for x in df_fixtures['stage']]\n",
    "    \n",
    "    # replace fixture status by their ids - all fixtures are already completed for that exercize\n",
    "    df_fixtures['statusid'] = dic_fixturestatus[fixturestatus]\n",
    "    \n",
    "    # format teams\n",
    "    df_fixtures['home_team_formated'] = [format_team(x) for x in df_fixtures['team_home']]\n",
    "    df_fixtures['away_team_formated'] = [format_team(x) for x in df_fixtures['team_away']]\n",
    "    \n",
    "    # replace teams by their ids\n",
    "    df_fixtures['hometeamid'] = [dic_teams[x] for x in df_fixtures['home_team_formated']]\n",
    "    df_fixtures['awayteamid'] = [dic_teams[x] for x in df_fixtures['away_team_formated']]\n",
    "    \n",
    "    # create index table\n",
    "    df_fixtures['id'] = [x for x in range(fixtures_max_index, fixtures_max_index + df_fixtures.shape[0])]\n",
    "    df_fixtures = df_fixtures.set_index('id')    \n",
    "    \n",
    "    # rename columns to match database\n",
    "    df_fixtures['date'] = df_fixtures['date']\n",
    "    df_fixtures['homescore'] = df_fixtures['score_home']\n",
    "    df_fixtures['awayscore'] = df_fixtures['score_away']\n",
    "    \n",
    "    # keep only a few columns\n",
    "    return df_fixtures[['date','competitionid','stageid','statusid','hometeamid','awayteamid','homescore','awayscore']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [db_insert_to_table] insert dataframe to table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def db_insert_to_table(df, table,index_col=''):\n",
    "    engine = create_engine('postgresql://{0}:{1}@{2}:{3}/{4}'.format(user, password, host, port, databasename))\n",
    "    if(index_col!=''):\n",
    "        df.to_sql(table, engine, if_exists='append',index=index_col)\n",
    "    else:\n",
    "        df.to_sql(table, engine, if_exists='append',index=False)\n",
    "    engine.dispose();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [db_process_histo_insert] load all histo csv files into dataframe and insert it to database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def db_process_histo_insert(root_path, competitionname, fixturestatus):\n",
    "    # load all games data from histo csv files into a dataframe\n",
    "    df_fixtures = histo_csv_to_df(root_path)\n",
    "\n",
    "    # connect to database\n",
    "    conn = db_connect()\n",
    "    cur = conn.cursor()\n",
    "    \n",
    "    # first we need to insert competitions\n",
    "    dic_seasons = db_seasons_table_to_dic(cur)\n",
    "    dic_allcompetitions = db_allcompetitions_table_to_dic(cur)\n",
    "    df_competitions_to_insert = get_competitions_to_df(df_fixtures, dic_seasons, dic_allcompetitions[competitionname])\n",
    "    \n",
    "    # insert competitions to database\n",
    "    db_insert_to_table(df_competitions_to_insert, 'competitions', 'id')\n",
    "    print('competitions inserted to db..')\n",
    "    \n",
    "    # get other main tables info\n",
    "    seasons = db_get_seasons(cur)\n",
    "    stages = db_get_stages(cur)\n",
    "    dic_competitions = db_competitions_table_to_dic(cur, competitionname)\n",
    "    dic_stages = db_stages_table_to_dic(cur)\n",
    "    dic_fixturestatus = db_fixturestatus_table_to_dic(cur)\n",
    "    dic_teams = db_teams_table_to_dic(cur)\n",
    "    fixtures_max_index = db_get_fixtures_max_index(cur)\n",
    "    \n",
    "    # close database\n",
    "    conn.close()\n",
    "    \n",
    "    # normalize fixtures data\n",
    "    df_normalized = normalize_data(df_fixtures, dic_competitions, dic_stages, dic_fixturestatus, fixturestatus, dic_teams, fixtures_max_index)\n",
    "    \n",
    "    # insert fixtures data to database\n",
    "    db_insert_to_table(df_normalized, 'fixtures', 'id')\n",
    "    print('{0} fixtures inserted to db..'.format(competitionname))\n",
    "    \n",
    "    # registered teams for each competitions\n",
    "    df_registeredteams = get_registered_teams(df_normalized, dic_stages)\n",
    "    \n",
    "    # insert registered teams to database\n",
    "    db_insert_to_table(df_registeredteams, 'registeredteams')\n",
    "    print('{0} registered teams inserted to db..'.format(competitionname))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "competitions inserted to db..\n",
      "Ligue 1 fixtures inserted to db..\n",
      "Ligue 1 registered teams inserted to db..\n"
     ]
    }
   ],
   "source": [
    "## TEST DATABASE INSERTION ##\n",
    "## LIGUE 1 ##\n",
    "#db_process_histo_insert(csv_root_path_ligue1, 'Ligue 1', 'Completed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## TEST week unique end dates ##\n",
    "# connect to database\n",
    "#conn = db_connect()\n",
    "#cur = conn.cursor()\n",
    "#competition = 'Ligue 1'\n",
    "#season = '1988/1989'\n",
    "#df = get_week_unique_dates_to_df(cur, competition, season)\n",
    "#conn.close()\n",
    "\n",
    "#process_seasons_week_unique_dates('Ligue 1')\n",
    "\n",
    "# close log file objects\n",
    "#logger.removeHandler(hdlr)\n",
    "#del logger,hdlr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test dataframe shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#dtypeCount =[df.iloc[:,i].apply(type).value_counts() for i in range(df.shape[1])]\n",
    "#dtypeCount"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
